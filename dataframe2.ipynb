{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({\n",
    "    'Name': ['John', 'Jane', 'Doe','Taj', 'Ravi', 'Anu'],\n",
    "    'Age': [28, 34, 45, 22, 30, 25],\n",
    "    'Experience': [5, 10, 15, 2, 8, 3],\n",
    "    'Salary': [50000, 60000, 70000, 40000, 55000, 45000],\n",
    "})\n",
    "df.to_csv('dataframe2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Practise_Dataframe\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----------+-------+\n",
      "|Name| Age|Experience| Salary|\n",
      "+----+----+----------+-------+\n",
      "|John|28.0|       5.0|50000.0|\n",
      "|Jane|34.0|      10.0|60000.0|\n",
      "| Doe|45.0|      15.0|70000.0|\n",
      "| Taj|22.0|       2.0|40000.0|\n",
      "|Ravi|30.0|       8.0|55000.0|\n",
      "| Anu|25.0|       3.0|45000.0|\n",
      "|NULL|NULL|       5.0|30000.0|\n",
      "|NULL|18.0|       1.0|12000.0|\n",
      "| Sam|40.0|      NULL|80000.0|\n",
      "|NULL|NULL|      NULL|   NULL|\n",
      "+----+----+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = spark.read.csv('dataframe2.csv', header=True, inferSchema=True)\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----------+-------+\n",
      "|Name| Age|Experience| Salary|\n",
      "+----+----+----------+-------+\n",
      "|John|28.0|       5.0|50000.0|\n",
      "|Jane|34.0|      10.0|60000.0|\n",
      "| Doe|45.0|      15.0|70000.0|\n",
      "| Taj|22.0|       2.0|40000.0|\n",
      "|Ravi|30.0|       8.0|55000.0|\n",
      "| Anu|25.0|       3.0|45000.0|\n",
      "+----+----+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop().show()\n",
    "# by default how='any'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----------+-------+\n",
      "|Name| Age|Experience| Salary|\n",
      "+----+----+----------+-------+\n",
      "|John|28.0|       5.0|50000.0|\n",
      "|Jane|34.0|      10.0|60000.0|\n",
      "| Doe|45.0|      15.0|70000.0|\n",
      "| Taj|22.0|       2.0|40000.0|\n",
      "|Ravi|30.0|       8.0|55000.0|\n",
      "| Anu|25.0|       3.0|45000.0|\n",
      "|NULL|NULL|       5.0|30000.0|\n",
      "|NULL|18.0|       1.0|12000.0|\n",
      "| Sam|40.0|      NULL|80000.0|\n",
      "+----+----+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop(how='all').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "thresh means atleast non null values should be present i that row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----------+-------+\n",
      "|Name| Age|Experience| Salary|\n",
      "+----+----+----------+-------+\n",
      "|John|28.0|       5.0|50000.0|\n",
      "|Jane|34.0|      10.0|60000.0|\n",
      "| Doe|45.0|      15.0|70000.0|\n",
      "| Taj|22.0|       2.0|40000.0|\n",
      "|Ravi|30.0|       8.0|55000.0|\n",
      "| Anu|25.0|       3.0|45000.0|\n",
      "|NULL|NULL|       5.0|30000.0|\n",
      "|NULL|18.0|       1.0|12000.0|\n",
      "| Sam|40.0|      NULL|80000.0|\n",
      "+----+----+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop(how='any',thresh=2).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----------+-------+\n",
      "|Name| Age|Experience| Salary|\n",
      "+----+----+----------+-------+\n",
      "|John|28.0|       5.0|50000.0|\n",
      "|Jane|34.0|      10.0|60000.0|\n",
      "| Doe|45.0|      15.0|70000.0|\n",
      "| Taj|22.0|       2.0|40000.0|\n",
      "|Ravi|30.0|       8.0|55000.0|\n",
      "| Anu|25.0|       3.0|45000.0|\n",
      "|NULL|NULL|       5.0|30000.0|\n",
      "|NULL|18.0|       1.0|12000.0|\n",
      "+----+----+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop(how='any',subset='Experience').show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----------+-------+\n",
      "|Name| Age|Experience| Salary|\n",
      "+----+----+----------+-------+\n",
      "|John|28.0|       5.0|50000.0|\n",
      "|Jane|34.0|      10.0|60000.0|\n",
      "| Doe|45.0|      15.0|70000.0|\n",
      "| Taj|22.0|       2.0|40000.0|\n",
      "|Ravi|30.0|       8.0|55000.0|\n",
      "| Anu|25.0|       3.0|45000.0|\n",
      "|   -|NULL|       5.0|30000.0|\n",
      "|   -|18.0|       1.0|12000.0|\n",
      "| Sam|40.0|      NULL|80000.0|\n",
      "|   -|NULL|      NULL|   NULL|\n",
      "+----+----+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.fill('-').show()\n",
    "# df_pyspark.na.fill('-',['Name', 'Age', 'Salary']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+-------+\n",
      "|   Name| Age|Experience| Salary|\n",
      "+-------+----+----------+-------+\n",
      "|   John|28.0|       5.0|50000.0|\n",
      "|   Jane|34.0|      10.0|60000.0|\n",
      "|    Doe|45.0|      15.0|70000.0|\n",
      "|    Taj|22.0|       2.0|40000.0|\n",
      "|   Ravi|30.0|       8.0|55000.0|\n",
      "|    Anu|25.0|       3.0|45000.0|\n",
      "|Unknown| 0.0|       5.0|30000.0|\n",
      "|Unknown|18.0|       1.0|12000.0|\n",
      "|    Sam|40.0|       0.0|80000.0|\n",
      "|Unknown| 0.0|       0.0|    0.0|\n",
      "+-------+----+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.fill({\n",
    "    'Name': 'Unknown',\n",
    "    'Age': 0,\n",
    "    'Experience': 0,\n",
    "    'Salary': 0\n",
    "}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols=['Age', 'Experience', 'Salary'],\n",
    "    outputCols=[\"{}_imputed\".format(col) for col in ['Age', 'Experience', 'Salary']]\n",
    "    ).setStrategy('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----------+-------+-----------+------------------+-----------------+\n",
      "|Name| Age|Experience| Salary|Age_imputed|Experience_imputed|   Salary_imputed|\n",
      "+----+----+----------+-------+-----------+------------------+-----------------+\n",
      "|John|28.0|       5.0|50000.0|       28.0|               5.0|          50000.0|\n",
      "|Jane|34.0|      10.0|60000.0|       34.0|              10.0|          60000.0|\n",
      "| Doe|45.0|      15.0|70000.0|       45.0|              15.0|          70000.0|\n",
      "| Taj|22.0|       2.0|40000.0|       22.0|               2.0|          40000.0|\n",
      "|Ravi|30.0|       8.0|55000.0|       30.0|               8.0|          55000.0|\n",
      "| Anu|25.0|       3.0|45000.0|       25.0|               3.0|          45000.0|\n",
      "|NULL|NULL|       5.0|30000.0|      30.25|               5.0|          30000.0|\n",
      "|NULL|18.0|       1.0|12000.0|       18.0|               1.0|          12000.0|\n",
      "| Sam|40.0|      NULL|80000.0|       40.0|             6.125|          80000.0|\n",
      "|NULL|NULL|      NULL|   NULL|      30.25|             6.125|49111.11111111111|\n",
      "+----+----+----------+-------+-----------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputer.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (PySpark)",
   "language": "python",
   "name": "pyspark-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
